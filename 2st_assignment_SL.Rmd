---
title: "Part 1: Statistical Tools"
author: "Ignacio AlmodÃ³var"
date: "12/11/2021"

output: 
  html_document:
    toc:true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

```{r,echo=FALSE,message=FALSE,cache=FALSE}
library(dplyr)
library(caret)
library(ggplot2)
library(klaR)
library(GGally)
library(tidyverse)
library(MASS)
library(caret)
library(VGAM)
library(e1071)
library(ROSE)
library(rpart.plot)
library(randomForest)
```

# First Step

## Download Data

The data chosen for this study has been taken from kaggle. It contains several financial ratios from companies and a column that specifies if the company is in bankruptcy or not.

The data were collected from the Taiwan Economic Journal for the years 1999 to 2009. Company bankruptcy was defined based on the business regulations of the Taiwan Stock Exchange.

```{r}
setwd("/Users/ignacioalmodovarcardenas/Desktop/Statistical Learning/Statistical_Learning/")
data=read.csv("data.csv")
data$Bankrupt.=as.factor(data$Bankrupt.)
data$Liability.Assets.Flag=as.factor(data$Liability.Assets.Flag)
dim(data)
```

We can see that there are 96 different variables. Working with this amount of variables is going to be practically impossible both in terms of understanding of the data set and about computation times. Therefore, I made a brief study about finance and found out which ratios are more useful to find whereas a company is in bankruptcy or not.

- Bankrupt: Binary class, 1 for bankruptcy 0 for not.
- ROAC: Return on total assets before interest and depreciation.
- Effective Tax Rate
- Total debt/Total net worth
- Debt ratio %: Liability/Total Assets
- Operating profit/Paid-in capital
- Cash/Total Assets
- Cash Flow to Equity
- Liability-Assets Flag: 1 if Total Liability exceeds Total Assets, 0 otherwise

```{r,echo=FALSE}
data_1 = data %>% dplyr::select(c(1,2,16,38,43,58,84,86))
names(data_1)
```

## Pre-process and Analyze

First of all I wanted to check if there are any missing values NA in my data set. In order to do that I wrote a function that specifies if there are any missing values, and if so, it says the number of missing values and the column that contains it.

```{r}
missingValues=function(data){
  count=0
  a=cbind(lapply(lapply(data, is.na), sum))
  for(i in 1:ncol(data)){
    if(a[i]!=0){
      cat("There are", a[i], "missing values in column ", i,"\n" )
      count=count+1
    }
  }  
    if(count==0){
      cat("There are No missing values in this dataset")
    }
}

missingValues(data)
```

Also, part of the pre-process we are going to split the dataset in training and testing sets.

```{r}
set.seed(1234)
index1=createDataPartition(y = data_1$Bankrupt.,p=0.6,list = FALSE)
trainSet=data_1[index1,]
testSet=data_1[-index1,]
```

Now, looking for differentiation in groups and other characteristics I am going to look for more information about which variables are the best predictors.

Therefore we are mainly going to use density plot in order to know if the variables might me good for prediction or not.

```{r}
trainSet %>% ggplot(aes(x =ROA.C..before.interest.and.depreciation.before.interest)) +  
  geom_density(aes( colour = Bankrupt., fill = Bankrupt.),alpha = 0.2)
```

We can see that using ROAC there is a slight difference on the two populations (Bankcruptcy o not), so we might use this variable for our prediction model.

```{r}
trainSet %>% ggplot(aes(x =Cash.Flow.to.Equity)) +  
  geom_density(aes( colour = Bankrupt., fill = Bankrupt.),alpha = 0.2)
```

Using this same process we can conclude that this variable is not going to be a god predictor by herself for our model, as both populations behaviors are quite similar.

```{r}
trainSet %>% ggplot(aes(x =Debt.ratio..)) +  
  geom_density(aes( colour = Bankrupt., fill = Bankrupt.),alpha = 0.2)

trainSet %>% ggplot(aes(x =Cash.Total.Assets)) +  
  geom_density(aes( colour = Bankrupt., fill = Bankrupt.),alpha = 0.2)

trainSet %>% ggplot(aes(x =Operating.profit.Paid.in.capital)) +  
  geom_density(aes( colour = Bankrupt., fill = Bankrupt.),alpha = 0.2)

trainSet %>% ggplot(aes(x =Tax.rate..A.)) +  
  geom_density(aes( colour = Bankrupt., fill = Bankrupt.),alpha = 0.2)

```
Within all this plots we can also say that the variable "Debt.ratio" can also be a good predictor for companies in bankruptcy.

In order to compare binary variables I decided to do barplots.

```{r}
ggplot(trainSet,aes(x=Liability.Assets.Flag,color=Bankrupt.,fill=Bankrupt.)) + geom_bar(size=2)
```

Within this plot you can see that the variable "Liability.Assets.Flag", is a good predictor for companies that are in Bankruptcy, as all of them have this flag, whereas only a few percentage of them that are not in Bankruptcy have this flag.

However, one of the problems in this dataset is that our target variable Bankruptcy is not so well balanced, as the number of observations in bankruptcy is way lower than the ones that are not. 

```{r}
as.integer(table(trainSet$Bankrupt.)[2])/as.integer(table(trainSet$Bankrupt.)[1])*100
```

We can see that only a 3.3% of the observations are in bankruptcy. As it is quite useless to make predictive models with datasets too unbalanced, we are going to make our training data balanced.

```{r}
data_balanced_train <- ovun.sample(Bankrupt. ~ ., data = trainSet, method = "over",N = 6092)$data
table(data_balanced_train$Bankrupt.)
```

Now let's create again a train and a control set.

```{r}
index2=createDataPartition(y = data_balanced_train$Bankrupt.,p=0.8,list = FALSE)
trainSet2=data_balanced_train[index2,]
testSet2=data_balanced_train[-index2,]
```

It is also very useful to do some boxplots in order to see if our data has large number of outliers or not.

```{r}
ggplot(trainSet2,aes(x=Bankrupt.,
                y = ROA.C..before.interest.and.depreciation.before.interest)) + 
  geom_boxplot()
```

As it can be seen there are a few of them, therefore we are going to save an index of outliers just in case I need to remove it in any case of the analysis.

```{r}
outliers <- boxplot(trainSet2$ROA.C..before.interest.and.depreciation.before.interest,
                    plot=FALSE)$out
```

Another way to find which variables might be good predictors we can use pairs and see the correlation between the different groups.

```{r}
pairs(trainSet2[,2:7],pch=21,col=c("blue","orange2")[trainSet2$Bankrupt.])
```

As it can be seen, most of the variables do split the observations in two groups. Even though there is no linear relationship between any of them, all of them differentiates the bankruptcy observations in one extreme of the scatter plot. Indeed we still can see that the best continuous predictors are the variables ROAC and Debt.ratio.

Other useful plots to visualize classification based on predictors are the ones given by QDA and LDA

```{r}
partimat(Bankrupt. ~ Debt.ratio.. + ROA.C..before.interest.and.depreciation.before.interest + 
           Cash.Total.Assets ,data=trainSet2,method="lda",)
```

Now using QDA with the same variables:

```{r}
partimat(Bankrupt. ~ Debt.ratio.. + ROA.C..before.interest.and.depreciation.before.interest + 
           Cash.Total.Assets ,data=trainSet2,method="qda",)
```

As it can be seen, these variables are actually not bad at all predictors using these methods. Even thought there are a lot of points that are not being well-classified, this methods have potential and we could use them to predict our data

## Classification modeling

As our target is a binary variable, we are going to first start building our model by using Logistic Regression.

```{r}
log.fit = vglm(Bankrupt. ~ Debt.ratio.., family=multinomial(refLevel=1), data=trainSet2)
prob.test = predict(log.fit, newdata=testSet2, type="response")
pred.test <- as.factor(levels(testSet2$Bankrupt.)[max.col(prob.test)])
ConfMat = table(pred.test,testSet2$Bankrupt.)
ConfMat

n = length(testSet2$Bankrupt.)
prop.errors <- (n - sum(diag(ConfMat))) / n
prop.errors

accuracy <- sum(diag(ConfMat)) / n
accuracy
```

Building our logistic model with just one of our variables already gives a pretty good accuracy rate. However, the error is still quite high. We have to try to make the model being able to predict more companies that are in bankruptcy as it.

As the results obtained are promising, I am going to carry on the modeling using logistic regression, but now using all the variables in our training set.

```{r}
log.fit = vglm(Bankrupt. ~ ., family=multinomial(refLevel=1), data=trainSet2)
prob.test = predict(log.fit, newdata=testSet2, type="response")
pred.test <- as.factor(levels(testSet2$Bankrupt.)[max.col(prob.test)])
confusionMatrix(pred.test,testSet2$Bankrupt.)
```

The model obtained is actually pretty good already, both Kappa and Accuracy indicators have a good rate.

Now, let's try using the functions included in the library CARET. We can use a cross validation algorithm to improve our prediction. In addition, as we want to improve the kappa and not the accuracy due to the fact that our data is not perfectly balanced, we can specify it in the metric.

```{r,warning=FALSE}
ctrl <- trainControl(method = "repeatedcv", repeats = 4,number = 7)
lrFit <- train(Bankrupt. ~ ., 
                method = "glmnet",
                family = "binomial",
                data = testSet2,
                preProcess = c("center","scale"),
                tuneGrid = expand.grid(alpha = seq(0, 2, 0.1), lambda = seq(0, .1, 0.01)),
                metric = "Kappa",
                trControl = ctrl)
```


```{r}
lrPred = predict(lrFit, testSet2)
confusionMatrix(lrPred, testSet2$Bankrupt.)
```

As you can see, the prediction model obtained is very promising and behaves quite good. Nevertheless we are now going to build other models with different methods seen in class to see if we can also build good predictions with them.

First of all we are going to use LDA.

```{r}
lda.model <- lda(Bankrupt. ~ ., data=trainSet2,prior=c(0.5,0.5))
probability = predict(lda.model, newdata=testSet2)$posterior
prediction = predict(lda.model, newdata=testSet2)$class
confusionMatrix(prediction, testSet2$Bankrupt.)
```

Actually the accuracy obtained is quite surprising. Nevertheless we can see that again, the kappa is quite low and it directly affects to the amount of companies in bankruptcy predicted.

Trying with caret and using the cross validation method the model might improve.

```{r}
ldaFit <- train(Bankrupt. ~ ., 
                method = "lda", 
                data = trainSet2,
                preProcess = c("center", "scale"),
                metric = "Accuracy",
                trControl = ctrl)

ldaPred = predict(ldaFit, testSet2)
confusionMatrix(ldaPred,testSet2$Bankrupt.)
```

Surprisingly, this model is the best one so far, as it has both accuracy and kappa rates slightly higher than the ones obtained with the logistic regression.

Now, let's try with QDA

```{r}
qda.model <- qda(Bankrupt. ~ ., data=trainSet2,prior=c(0.5,0.5))
prediction = predict(qda.model, newdata=testSet2)$class
confusionMatrix(prediction, testSet2$Bankrupt.)
```

Using caret:

```{r,warning=FALSE}
qdaFit <- train(Bankrupt. ~ ., 
                method = "qda", 
                data = trainSet2,
                preProcess = c("center", "scale"),
                metric = "Accuracy",
                trControl = ctrl)
```


```{r}
qdaPred = predict(qdaFit, testSet2)
confusionMatrix(qdaPred,testSet2$Bankrupt.)
```

The results obtained are not bad at all, however, I have been obtaining better predictions with the models used before. Therefore, we can conclude that QDA is not a good method to predict for this dataset.

Finally, let's try with Naive Bayes.

```{r}
naive.model <- naiveBayes(Bankrupt. ~ ., data=trainSet2, laplace=1, prior = c(0.5,0.5))
prediction = predict(naive.model, newdata=testSet2)
confusionMatrix(prediction, testSet2$Bankrupt.)
```

And using caret:

```{r,warning=FALSE}
nbFit <- train(Bankrupt. ~ ., 
                method = "nb", 
                data = trainSet2,
                preProcess = c("center", "scale"),
                metric = "Accuracy",
                trControl = ctrl)
```

```{r,warning=FALSE}
nbPred = predict(nbFit, testSet2)
confusionMatrix(nbPred, testSet2$Bankrupt.)
```

Again, the results obtained are pretty good, but we have already obtained better results both with LDA and logistic regression.

I have already built several models and even though each of them had their pros and cons, I'm going to stick with the one built using LDA in caret.

```{r}
confusionMatrix(ldaPred,testSet2$Bankrupt.)
```

Nevertheless, there is still a fact that bothers me in this model. It is the fact that the error rate for predicting companies in Bankruptcy is still quite high. From an investor point of view, this model will be quite useless, as it is predicting companies that are actually in bankruptcy wrong. Therefore, I need to upgrade this model in that aspect. This can be done changing the threshold that acts like a boundary for errors.

```{r}
lrProb = predict(ldaFit, newdata=testSet2, type="prob")
threshold = 0.4
lrPred = rep("0", nrow(testSet2))
lrPred[which(lrProb[,2] > threshold)] = "1"
lrPred = factor(lrPred)
confusionMatrix(lrPred, testSet2$Bankrupt.)
```

As it can bee seen, for the same model we get different results as the threshold moves. Now in order to find an optimal threshold we can run this code.

```{r}
relative.cost <- c(1, 2, 0.5, 0.0)
CM = confusionMatrix(lrPred, testSet2$Bankrupt.)$table
sum(relative.cost*CM)
cost.i = matrix(NA, nrow = 10, ncol = 9)

j <- 0


for (threshold in seq(0.35,0.75,0.05)){

  j <- j + 1

  for(i in 1:10){

    # partition data intro training (80%) and testing sets (20%)
    d <- createDataPartition(trainSet2$Bankrupt., p = 0.8, list = FALSE)
    
    train<-trainSet2[d,]
    test <-trainSet2[-d,]  
    
    ldaFit <- train(Bankrupt. ~ ., 
                    method = "lda", 
                    data = train,
                    preProcess = c("center", "scale"),
                    metric = "Accuracy",
                    trControl = ctrl)
    lrProb = predict(ldaFit, test, type="prob")
    
    lrPred = rep(0, nrow(test))
    lrPred[which(lrProb[,2] > threshold)] = 1
    lrPred = factor(lrPred)
    
    CM = confusionMatrix(lrPred, test$Bankrupt.)$table
    
    cost.i[i,j] <- sum(relative.cost*CM)/nrow(test) # unitary cost

    
  }
}

boxplot(cost.i, main = "Hyper-parameter selection",
        ylab = "cost",
        xlab = "threshold value",names = seq(0.35,0.75,0.05),col="lightblue")
```
From our point of view, as we are looking for having less error in the prediction of Bankruptcy companies, our optimal threshold is 0.35. If we wanted to get lower error on predicting companies that are not in Bankruptcy we will probably choose a threshold around 0.5 and 0.55.

Within this we can finally build our best model.

```{r}
# optimal threshold
threshold = 0.35

# final prediction
ldaFit <- train(Bankrupt. ~ ., 
                method = "sparseLDA", 
                data = trainSet2,
                preProcess = c("center", "scale"),
                metric = "Accuracy",
                trControl = ctrl)
ldaPred = predict(ldaFit, testSet2)

lrProb = predict(ldaFit, newdata=testSet2, type="prob")
lrPred = rep(0, nrow(testSet2))
lrPred[which(lrProb[,2] > threshold)] = 1
lrPred = factor(lrPred)
confusionMatrix(lrPred,testSet2$Bankrupt.)
```

## Conclusions 1st step

Working with this dataset has not been an easy thing. There were a few things that made it difficult and the main one was the fact that our target variable was Imbalanced, and not just a little bit. In this case the proportion was 97% of observations that were not in bankruptcy whereas only a 3% of them actually were.

When I first started working on the dataset (mainly in prepossess), I did not really know much about classification, therefore I didn't thought that it was going to be a problem having this difference in proportions. Later, when I started working on the predictive models I realized that the dataset was not being very useful, due the fact that on every model the accuracy was over 0.96, as most of the observations were predicting as not being in bankruptcy and only a few of them were (between 6 and 9 observations). 

As I have already worked a lot in this dataset, I could not start over with another different dataset so I focused my models on being able to get higher kappa, sensitivity, specificity and precision rates instead of focusing in the accuracy, which I really did not care about. 

Then looking through different resources I found our about different methods to balance the data, and I apply them to this dataset, what brought me the possibility to build good classification models.

# 2nd Step Machine Learning methods

On this second step of the project we are going to use different Machine Learning methods to obtain our predictions about if a company is in bankruptcy or not. 

We will test:

- Nearest Neighbors
- SVMs
- Decision trees 
- Random Forest 
- Gradient Boosting
- Neural Networks

## Nearest Neighbors

First of all we are going to test how well does knn work with our dataset. This method is very useful for non-linear data, which is our case. Nevertheless it works worse on data sets with high dimension. We are also going to use cross validation over it with the same train control selected in the first step,  a "repeatedcv" for 4 times.

Due to the final goal of our analysis I will keep the kappa as an accuracy method. In order to use the optimal method we will do hyperparameter tunning, so we know that the model used is the best one for our data. Using the function in caret we can  already train our model with the optimal number of neighbors for the model.

```{r}
knnFit <- train(Bankrupt. ~ ., 
                method = "knn", 
                data = trainSet2,
                preProcess = c("center", "scale"),
                tuneLength = 5,
                metric = "Kappa",
                trControl = ctrl)
print(knnFit)
```

We can see that the best result is given when k=5, as both Accuracy and Kappa are the best ones obtained. This optimal k will be the one used to train our model.

```{r}
knnPred = predict(knnFit, testSet2)
confusionMatrix(knnPred,testSet2$Bankrupt.)
```

Given the confusion matrix for the prediction and the real values we can see that the results obtained are very promising, indeed, this is the best prediction obtained so far. We can see that we are obtained an accuracy of almost an 94% and the error of the prediction is only given in companies that are predicted as Bankruptcy that are not. This is the best case for this dataset as my goal as it will be much worse predicting a company that is in Bankruptcy as not than the other way.

Also, those companies that are being predicted as Bankruptcy could mean that even though they are not declared as Bankruptcy they might be very close to it, so giving a prediction of being Bankruptcy could not be that bad at all if we are seeing it from a investors point of view.


## Support Vector Machines

Now we are going to see how support vector machines works with our data. Again, we are going to do hyperparameter tunning in order to obtain the best parameters for our dataset. 

First we are going to see how good it works for non optimal parameters.

```{r}
svm.train <- svm(Bankrupt.~., data=trainSet2, scale=T, kernel="sigmoid",
                 gamma=0.01, cost=1)
svm.pred <- predict(svm.train, newdata=testSet2)
confusionMatrix(svm.pred, testSet2$Bankrupt.)
```

We can see that the results are not bad at all compared to the methods that we used in the first step. However, when compared to knn, the results are worse. Now we are going to see if we can make it better using hyperparameter tunning. First of all we are going to use Support Vector Machines with Radial Basis Function Kernel method.

```{r}
svmFit_rad <- train(Bankrupt. ~., method = "svmRadial", 
                data = trainSet2,
                preProcess = c("center", "scale"),
                tuneGrid = expand.grid(C = c(0.1, .25, .5, 1),
                                      sigma = c(0.01,.05,0.1)), 
                metric = "Kappa",
                trControl = ctrl)

svmRadPred = predict(svmFit_rad, testSet2)
confusionMatrix(svmRadPred,testSet2$Bankrupt.)
```

Again we can see that even though the accuracy is high, it is not as good as the one obtained by the knn method optimized. Both accuracy and kappa values are slightly higher than the ones obtained with SVM Radial not optimized.

## Decission Trees

As SVM did not workout as we wanted to we are going to try now with decision trees. Again, we are going to test a model optimized and another one with hyperparameter tunning.

```{r}
rpart.fit <- rpart(Bankrupt. ~., method="class", data = trainSet2)
rpart.pred <- predict(rpart.fit, newdata=testSet2,type="class")
confusionMatrix(rpart.pred, testSet2$Bankrupt.)
```

Using an non optimized decision tree we can see that the accuracy has gotten higher. However, it is still lower than the one obtained with knn. Even though it is not the best model for our data it works pretty good and one of the benefits of using it is that you can easily get the scheme of how the model classifies companies as bankruptcy or not and get a better understanding of it.

```{r}
rpart.plot(rpart.fit, digits = 3, fallen.leaves = TRUE,
           type = 3, extra=101)
```

Now we are going to build a model based on decision trees with hyperparameter tunning.

```{r}
grid_c50 <- expand.grid( .winnow = c(TRUE,FALSE), .trials=c(1,5,10,15,20), .model="tree" )

fit.c50 <- train(Bankrupt. ~.,
                data=trainSet2,
                method="C5.0",
                metric="Kappa",
                tuneGrid = grid_c50,
                trControl = ctrl)
print(fit.c50)

c50.pred <- predict(fit.c50, newdata=testSet2)
confusionMatrix(c50.pred, testSet2$Bankrupt.)
```

Surprisingly the accuracy obtained in this model is almost 99%! Also the Kappa is very high, 97%. The model has been fitted using 20 trials and window as True. This numbers are going to be very difficult to improve as it is making almost a perfect classification, not only due to the accuracy but also to the fact that again it is not predicting any companies that are in Bankruptcy as not, which is one of the most important goals of our analysis.


## Random Forest

We are now going to test Random Forest and see if we get better or worse results than in the previous models. As we have been doing, we will first built a model without tuning parameters and another one tuning them.  

```{r,results=FALSE}
rf.train <- randomForest(Bankrupt. ~., data=trainSet2,
                         ntree=250,mtry=10,cutoff=c(0.5,0.5),importance=TRUE, do.trace=T)
```
```{r}
rf.pred <- predict(rf.train, newdata=testSet2)
confusionMatrix(rf.pred, testSet2$Bankrupt.)
```

Again the results obtained are slightly better than the ones obtained with decision trees. 


```{r}
rf.train <- train(Bankrupt. ~., 
                  method = "rf", 
                  data = trainSet2,
                  preProcess = c("center", "scale"),
                  ntree = 250,
                  cutoff=c(0.5,0.5),
                  tuneGrid = expand.grid(mtry = c(10,20,30)), 
                  metric = "Kappa",
                  trControl = ctrl)
print(rf.train)
rf.pred <- predict(rf.train, testSet2)
confusionMatrix(rf.pred, testSet2$Bankrupt.)
```

```{r}
rf.pred2 <- predict(rf.train, testSet)
confusionMatrix(rf.pred2, testSet$Bankrupt.)
```



## Gradient Boosting


```{r}
GBM.train <- gbm(Bankrupt.~., data=trainSet2,
                 distribution= "bernoulli",n.trees=250,shrinkage = 0.01,interaction.depth=2,n.minobsinnode = 8)
threshold = 0.2
gbmProb = predict(GBM.train, newdata=testSet2, n.trees=250, type="response")
gbmPred = rep("No", nrow(testSet2))
gbmPred[which(gbmProb > threshold)] = "Yes"
CM = confusionMatrix(factor(gbmPred), testing$Churn)$table
cost = sum(as.vector(CM)*cost.unit)/sum(CM)
cost
```




# Now optimize the hyper-parameters with Caret:

```{r}
xgb_grid = expand.grid(
  nrounds = c(500,1000),
  eta = c(0.01, 0.001), # c(0.01,0.05,0.1)
  max_depth = c(2, 4, 6),
  gamma = 1,
  colsample_bytree = c(0.2, 0.4),
  min_child_weight = c(1,5),
  subsample = 1
)

xgb.train = train(Bankrupt. ~ .,  data=testSet2,
                  trControl = ctrl,
                  metric="Kappa",
                  maximize = F,
                  tuneGrid = xgb_grid,
                  preProcess = c("center", "scale"),
                  method = "xgbTree"
)

xgb.pred <- predict(xgb.train, testSet)
confusionMatrix(xgb.pred, testSet$Bankrupt.)
```

```{r}
# Variable importance
xgb_imp <- varImp(xgb.train, scale = F)
plot(xgb_imp, scales = list(y = list(cex = .95)))
```


# ```{r}
# threshold = 0.2
# xgbProb = predict(xgb.train, newdata=testSet, type="prob")
# xgbPred = rep(0, nrow(testSet))
# xgbPred[which(xgbProb[,2] > threshold)] = 1
# CM = confusionMatrix(factor(xgbPred), testSet$Bankrupt.)$table
# cost = sum(as.vector(CM)*cost.unit)/sum(CM)
# cost
# ```


# not too good, because we need to improve the grid but too expensive...

# Even more advanced ideas:
# instead of using a fixed cost matrix for each customer, we could consider a 
# different matrix for each, depending on the value of each customer for the company

## Neural Networks

```{r}
dnn.train <- train(Bankrupt. ~., 
                  method = "dnn", 
                  data = trainSet2,
                  preProcess = c("center", "scale"),
                  numepochs = 20, # number of iterations on the whole training set
                  tuneGrid = expand.grid(layer1 = 1:4,
                                         layer2 = 0:2,
                                         layer3 = 0:2,
                                         hidden_dropout = 0, 
                                         visible_dropout = 0),
                  metric = "kappa",
                  maximize = F,
                  trControl = ctrl)


xgb.pred <- predict(xgb.train, testSet2)
confusionMatrix(xgb.pred, testSet2$Bankrupt.)
```



```{r}
relative.cost <- c(1, 2, 0.5, 0.0)
CM = confusionMatrix(xgb.pred, testSet$Bankrupt.)$table
sum(relative.cost*CM)
cost.i = matrix(NA, nrow = 10, ncol = 9)

j <- 0


for (threshold in seq(0.35,0.75,0.05)){

  j <- j + 1

  for(i in 1:10){

    # partition data intro training (80%) and testing sets (20%)
    d <- createDataPartition(trainSet$Bankrupt., p = 0.8, list = FALSE)
    
    train<-trainSet[d,]
    test <-trainSet[-d,]  
    
    dnn.train <- train(Bankrupt. ~., 
                  method = "dnn", 
                  data = trainSet,
                  preProcess = c("center", "scale"),
                  numepochs = 20, # number of iterations on the whole training set
                  tuneGrid = expand.grid(layer1 = 1:4,
                                         layer2 = 0:2,
                                         layer3 = 0:2,
                                         hidden_dropout = 0, 
                                         visible_dropout = 0),
                  metric = "kappa",
                  maximize = F,
                  trControl = ctrl)
    
    lrProb = predict(dnn.train, test, type="prob")
    
    lrPred = rep(0, nrow(test))
    lrPred[which(lrProb[,2] > threshold)] = 1
    lrPred = factor(lrPred)
    
    CM = confusionMatrix(lrPred, test$Bankrupt.)$table
    
    cost.i[i,j] <- sum(relative.cost*CM)/nrow(test) # unitary cost

    
  }
}

boxplot(cost.i, main = "Hyper-parameter selection",
        ylab = "cost",
        xlab = "threshold value",names = seq(0.35,0.75,0.05),col="lightblue")
```



```{r}
threshold = 0.45
dnnProb = predict(dnn.train, newdata=testSet, type="prob")
dnnPred = rep(0, nrow(testSet))
dnnPred[which(dnnProb[,2] > threshold)] = 1
confusionMatrix(as.factor(dnnPred),testSet$Bankrupt.)
#EconomicCost(data = data.frame(pred  = dnnPred, obs = testing$Churn))
```


